{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55333c4",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35b13b",
   "metadata": {},
   "source": [
    "# ConversationBufferMenory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4e46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_968\\4132656500.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#애는 캐쉬와 다르게 그냥 대화 내역을 저장 (캐쉬는 대답 저장)\n",
    "# memory = ConversationBufferMemory()\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c1d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\" : \"Hi\"}, {\"output\" : \"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86947e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce97d38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c63a4b",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMenory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6808bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_968\\932432625.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True\n",
    "    ,k=5\n",
    ")\n",
    "\n",
    "def addMessage(input, output):\n",
    "    memory.save_context({\"input\": input},{\"output\": output})\n",
    "\n",
    "addMessage(\"1\",\"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78db12ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    addMessage(str(i), str(i))\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# 메모리 관리가 되지만 최근 내역만 기억할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e6d9d",
   "metadata": {},
   "source": [
    "# ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3339eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model = \"gpt-4o-mini\",\n",
    "  temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\" : input}, {\"output\" : output})\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm deaHyun, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3db000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "addMessage(\"sung-il information high school is so suck\",\"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e4fc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as deaHyun from South Korea, and the AI responds positively, expressing that it is cool. The human then expresses dissatisfaction with Sung-il Information High School, to which the AI responds that it wishes it could attend.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03de6",
   "metadata": {},
   "source": [
    "# conversationKGMemory\n",
    "uv add networkx가 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9af54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt4o-mini\",temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm = llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def addMessage(input, output):\n",
    "    memory.save_context({\"input\": input},{\"output\": output})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9377b3",
   "metadata": {},
   "source": [
    "# Memory on LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc71191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_968\\2908350678.py:19: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n",
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_968\\2908350678.py:27: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM = off the shlf : general Perpose chain (일반적이 목적을 가지는 체인)\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "you ate a helpful AI taling a Human\n",
    "{chat_history}\n",
    "Human : {question}\n",
    "Your :\n",
    "\"\"\"\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    # prompt = PromptTemplate.from_template(\"{question}\")\n",
    "    prompt = PromptTemplate.from_template(template=template)\n",
    ")\n",
    "\n",
    "chain.predict(question = \"I live in seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32d4bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You live in Seoul! It's a dynamic city known for its unique blend of tradition and modernity. What do you like most about living there?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"Where's do I live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912dd9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [SystemMessage(content='The human mentions that they live in Seoul.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"Where's do I live?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"You live in Seoul! It's a dynamic city known for its unique blend of tradition and modernity. What do you like most about living there?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee3449",
   "metadata": {},
   "source": [
    "# ChatBasedMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6e45a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_22340\\3295060422.py:26: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  # max_token_limit=80\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb31758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\n",
      "Human: Where's do I living\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It sounds like you might be asking about where you live in Seoul. Seoul is divided into several districts, each with its own unique character. Do you want to share which district or area you live in, or are you looking for information about a specific neighborhood?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"Where's do I living\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6f835",
   "metadata": {},
   "source": [
    "# LCEL Based Memory (Langchain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deef60ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_22340\\2404693214.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke({\n",
    "    \"question\": question\n",
    "  })\n",
    "  memory.save_context({\"input\" : question}, {\"output\" : result.content})\n",
    "  print(result)\n",
    "\n",
    "# chain.invoke(\n",
    "#   {\n",
    "#     # \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"],\n",
    "#     \"chat_history\" : load_memory(),\n",
    "#     \"question\" : \"My name is Jeseok\"\n",
    "#   }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a4dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, DEAHYUN! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 27, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CKiMJpPDjLtgZIk71Yqjw4SEIt0fc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f51099b7-e4ba-4df1-a82e-fd974d88b393-0' usage_metadata={'input_tokens': 27, 'output_tokens': 17, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# chain.invoke({\n",
    "#     \"chat_history\":memory.load_memory_variables({})[\"chat_history\"],\n",
    "#     \"question\":\"What is my name\"\n",
    "#     })\n",
    "\n",
    "invoke_chain(\"My name is DEAHYUN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64848b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is DEAHYUN. How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 56, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CKiMMTVy6ldxbwtu31UDq2kJFwVbC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--a1d0e122-71fc-48a9-9ca4-a088b11c3851-0' usage_metadata={'input_tokens': 56, 'output_tokens': 15, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedb256",
   "metadata": {},
   "source": [
    "# 과제 : 30일 까지 CLI에서 작동하는 챗봇 만들기\n",
    "# 힌트 : 안에 생긴게 너무 json처럼 생김 30일 까지\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-7day-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
