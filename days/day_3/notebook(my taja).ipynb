{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55333c4",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35b13b",
   "metadata": {},
   "source": [
    "# ConversationBufferMenory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4e46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_1452\\4132656500.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#애는 캐쉬와 다르게 그냥 대화 내역을 저장 (캐쉬는 대답 저장)\n",
    "# memory = ConversationBufferMemory()\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c1d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\" : \"Hi\"}, {\"output\" : \"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86947e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce97d38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c63a4b",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMenory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6808bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_16992\\1546057324.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True\n",
    "    ,k=5\n",
    ")\n",
    "\n",
    "def addMessage(input, output):\n",
    "    memory.save_context({\"input\": input},{\"output\": output})\n",
    "\n",
    "addMessage(\"1\",\"1\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78db12ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    addMessage(str(i), str(i))\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# 메모리 관리가 되지만 최근 내역만 기억할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e6d9d",
   "metadata": {},
   "source": [
    "# ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3339eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_1452\\3746421551.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_1452\\3746421551.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model = \"gpt-4o-mini\",\n",
    "  temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\" : input}, {\"output\" : output})\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm deaHyun, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3db000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "addMessage(\"sung-il information high school is so suck\",\"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e4fc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as deaHyun from South Korea, and the AI responds positively, expressing excitement about this information. The human then shares their negative feelings about Sung-il High School, to which the AI responds with enthusiasm, wishing they could attend.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03de6",
   "metadata": {},
   "source": [
    "# conversationKGMemory\n",
    "uv add networkx가 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9af54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt4o-mini\",temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm = llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def addMessage(input, output):\n",
    "    memory.save_context({\"input\": input},{\"output\": output})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9377b3",
   "metadata": {},
   "source": [
    "# Memory on LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc71191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_21092\\2908350678.py:19: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n",
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_21092\\2908350678.py:27: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM = off the shlf : general Perpose chain (일반적이 목적을 가지는 체인)\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "you ate a helpful AI taling a Human\n",
    "{chat_history}\n",
    "Human : {question}\n",
    "Your :\n",
    "\"\"\"\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    # prompt = PromptTemplate.from_template(\"{question}\")\n",
    "    prompt = PromptTemplate.from_template(template=template)\n",
    ")\n",
    "\n",
    "chain.predict(question = \"I live in seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32d4bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You live in Seoul! It's a dynamic city known for its unique blend of tradition and modernity. What do you like most about living there?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"Where's do I live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "912dd9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='I live in seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee3449",
   "metadata": {},
   "source": [
    "# ChatBasedMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e45a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_22340\\3295060422.py:26: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  # max_token_limit=80\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb31758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\n",
      "Human: Where's do I living\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It sounds like you might be asking about where you live in Seoul. Seoul is divided into several districts, each with its own unique character. Do you want to share which district or area you live in, or are you looking for information about a specific neighborhood?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"Where's do I living\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6f835",
   "metadata": {},
   "source": [
    "# LCEL Based Memory (Langchain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deef60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke({\"question\": question})\n",
    "  memory.save_context({\"input\" : question}, {\"output\" : result.content})\n",
    "  print(result)\n",
    "\n",
    "# chain.invoke(\n",
    "#   {\n",
    "#     # \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"],\n",
    "#     \"chat_history\" : load_memory(),\n",
    "#     \"question\" : \"My name is Jeseok\"\n",
    "#   }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e33d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종료하려면 exit 입력\n",
      "챗봇을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "def run_cli_chatbot():\n",
    "    print(\"종료하려면 exit 입력\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"챗봇을 종료합니다.\")\n",
    "            break\n",
    "        response = invoke_chain(user_input)\n",
    "        print(f\"AI: {response}\\n\")\n",
    "\n",
    "\n",
    "run_cli_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a4dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, DEAHYUN! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 27, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CL6H2fCjYw3QNP470Yr3yJM7vptVh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--79f706d8-0c96-4921-ba0b-acfb16e881c1-0' usage_metadata={'input_tokens': 27, 'output_tokens': 17, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# chain.invoke({\n",
    "#     \"chat_history\":memory.load_memory_variables({})[\"chat_history\"],\n",
    "#     \"question\":\"What is my name\"\n",
    "#     })\n",
    "\n",
    "invoke_chain(\"My name is DEAHYUN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64848b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Your name is DEAHYUN. If you have any other questions or topics you'd like to talk about, just let me know!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 117, 'total_tokens': 144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CL6HUvaqksVqvpBW5WUbmcKe76DbP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0e203725-ce62-48d6-80d1-feeea77c9115-0' usage_metadata={'input_tokens': 117, 'output_tokens': 27, 'total_tokens': 144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The human introduces themselves as DEAHYUN, and the AI responds by expressing pleasure in meeting them and asking how it can assist today.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is DEAHYUN. How can I help you today?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Your name is DEAHYUN. If there's anything else you'd like to discuss or ask, feel free!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Your name is DEAHYUN. If you have any other questions or topics you'd like to talk about, just let me know!\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"What is my name\")\n",
    "\n",
    "load_memory(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7c049",
   "metadata": {},
   "source": [
    "# 현재까지 설치한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4266e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "list =[\"aiohappyeyeballs         2.6.1\",\n",
    "\"aiohttp                  3.12.15\",\n",
    "\"aiosignal                1.4.0\",\n",
    "\"annotated-types          0.7.0\",\n",
    "\"anyio                    4.11.0\",\n",
    "\"anyio                    4.11.0\",\n",
    "\"audioop-lts              0.2.2\",\n",
    "\"certifi                  2025.8.3\",\n",
    "\"charset-normalizer       3.4.3\",\n",
    "\"colorama                 0.4.6\",\n",
    "\"discord.py               2.6.3\",\n",
    "\"distro                   1.9.0\",\n",
    "\"frozenlist               1.7.0\",\n",
    "\"greenlet                 3.2.4\",\n",
    "\"h11                      0.16.0\",\n",
    "\"httpcore                 1.0.9\",\n",
    "\"httpx                    0.28.1\",\n",
    "\"idna                     3.10\",\n",
    "\"jiter                    0.11.0\",\n",
    "\"jsonpatch                1.33\",\n",
    "\"jsonpointer              3.0.0\",\n",
    "\"langchain                0.3.27\",\n",
    "\"langchain-core           0.3.76\",\n",
    "\"langchain-openai         0.3.33\",\n",
    "\"langchain-text-splitters 0.3.11\",\n",
    "\"langsmith                0.4.31\",\n",
    "\"multidict                6.6.4\",\n",
    "\"networkx                 3.5\",\n",
    "\"openai                   1.109.1\",\n",
    "\"orjson                   3.11.3\",\n",
    "\"packaging                25.0\",\n",
    "\"pip                      25.2\",\n",
    "\"propcache                0.3.2\",\n",
    "\"pydantic                 2.11.9\",\n",
    "\"pydantic_core            2.33.2\",\n",
    "\"python-dotenv            1.1.1\",\n",
    "\"pytz                     2025.2\",\n",
    "\"PyYAML                   6.0.3\",\n",
    "\"regex                    2025.9.18\",\n",
    "\"requests                 2.32.5\",\n",
    "\"requests-toolbelt        1.0.0\",\n",
    "\"sniffio                  1.3.1\",\n",
    "\"SQLAlchemy               2.0.43\",\n",
    "\"tenacity                 9.1.2\",\n",
    "\"tiktoken                 0.11.0\",\n",
    "\"tqdm                     4.67.1\",\n",
    "\"typing_extensions        4.15.0\",\n",
    "\"typing-inspection        0.4.1\",\n",
    "\"urllib3                  2.5.0\",\n",
    "\"yarl                     1.20.1\",\n",
    "\"zstandard                0.25.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedb256",
   "metadata": {},
   "source": [
    "# 과제 : 30일 까지 CLI에서 작동하는 챗봇 만들기\n",
    "# 힌트 : 안에 생긴게 너무 json처럼 생김 30일 까지\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-7day-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
