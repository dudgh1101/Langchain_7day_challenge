{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55333c4",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35b13b",
   "metadata": {},
   "source": [
    "# ConversationBufferMenory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4e46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_1452\\4132656500.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#Ïï†Îäî Ï∫êÏâ¨ÏôÄ Îã§Î•¥Í≤å Í∑∏ÎÉ• ÎåÄÌôî ÎÇ¥Ïó≠ÏùÑ Ï†ÄÏû• (Ï∫êÏâ¨Îäî ÎåÄÎãµ Ï†ÄÏû•)\n",
    "# memory = ConversationBufferMemory()\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c1d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\" : \"Hi\"}, {\"output\" : \"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86947e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"hi\"},{\"output\":\"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce97d38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c63a4b",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMenory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6808bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_16992\\1546057324.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True\n",
    "    ,k=5\n",
    ")\n",
    "\n",
    "def addMessage(input, output):\n",
    "    memory.save_context({\"input\": input},{\"output\": output})\n",
    "\n",
    "addMessage(\"1\",\"1\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78db12ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    addMessage(str(i), str(i))\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨Í∞Ä ÎêòÏßÄÎßå ÏµúÍ∑º ÎÇ¥Ïó≠Îßå Í∏∞ÏñµÌï† Ïàò ÏûàÎã§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e6d9d",
   "metadata": {},
   "source": [
    "# ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3339eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_1452\\3746421551.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_1452\\3746421551.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model = \"gpt-4o-mini\",\n",
    "  temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\" : input}, {\"output\" : output})\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm deaHyun, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3db000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "addMessage(\"sung-il information high school is so suck\",\"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e4fc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as deaHyun from South Korea, and the AI responds positively, expressing excitement about this information. The human then shares their negative feelings about Sung-il High School, to which the AI responds with enthusiasm, wishing they could attend.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03de6",
   "metadata": {},
   "source": [
    "# conversationKGMemory\n",
    "uv add networkxÍ∞Ä ÌïÑÏöîÌï®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9af54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt4o-mini\",temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm = llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def addMessage(input, output):\n",
    "    memory.save_context({\"input\": input},{\"output\": output})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9377b3",
   "metadata": {},
   "source": [
    "# Memory on LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc71191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_21092\\2908350678.py:19: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n",
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_21092\\2908350678.py:27: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM = off the shlf : general Perpose chain (ÏùºÎ∞òÏ†ÅÏù¥ Î™©Ï†ÅÏùÑ Í∞ÄÏßÄÎäî Ï≤¥Ïù∏)\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "you ate a helpful AI taling a Human\n",
    "{chat_history}\n",
    "Human : {question}\n",
    "Your :\n",
    "\"\"\"\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    # prompt = PromptTemplate.from_template(\"{question}\")\n",
    "    prompt = PromptTemplate.from_template(template=template)\n",
    ")\n",
    "\n",
    "chain.predict(question = \"I live in seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32d4bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You live in Seoul! It's a dynamic city known for its unique blend of tradition and modernity. What do you like most about living there?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"Where's do I live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "912dd9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='I live in seoul', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee3449",
   "metadata": {},
   "source": [
    "# ChatBasedMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e45a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_22340\\3295060422.py:26: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  # max_token_limit=80\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb31758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\n",
      "Human: Where's do I living\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It sounds like you might be asking about where you live in Seoul. Seoul is divided into several districts, each with its own unique character. Do you want to share which district or area you live in, or are you looking for information about a specific neighborhood?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question = \"Where's do I living\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6f835",
   "metadata": {},
   "source": [
    "# LCEL Based Memory (Langchain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deef60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke({\"question\": question})\n",
    "  memory.save_context({\"input\" : question}, {\"output\" : result.content})\n",
    "  print(result)\n",
    "\n",
    "# chain.invoke(\n",
    "#   {\n",
    "#     # \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"],\n",
    "#     \"chat_history\" : load_memory(),\n",
    "#     \"question\" : \"My name is Jeseok\"\n",
    "#   }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI Chatbot (Ï¢ÖÎ£åÌïòÎ†§Î©¥ 'exit' ÏûÖÎ†•)\n",
      "content='Nice to meet you, Deahyun! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 204, 'total_tokens': 220, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CL6QuoxqMbG8SRO1B1ltfHQ6DhPa9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--555f8335-2bb8-412a-a713-dee1a0d04838-0' usage_metadata={'input_tokens': 204, 'output_tokens': 16, 'total_tokens': 220, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "AI: None\n",
      "\n",
      "content='Your name is Deahyun. How can I help you further?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 210, 'total_tokens': 224, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CL6ROEc0sX72hQZZcWtROIwyXXhv3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b7cc9bad-f98f-4842-a219-5323784fb075-0' usage_metadata={'input_tokens': 210, 'output_tokens': 14, 'total_tokens': 224, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "AI: None\n",
      "\n",
      "üëã Ï±óÎ¥áÏùÑ Ï¢ÖÎ£åÌï©ÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "def run_cli_chatbot():\n",
    "    print(\"Ï¢ÖÎ£åÌïòÎ†§Î©¥ exit ÏûÖÎ†•\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Ï±óÎ¥áÏùÑ Ï¢ÖÎ£åÌï©ÎãàÎã§.\")\n",
    "            break\n",
    "        response = invoke_chain(user_input)\n",
    "        print(f\"AI: {response}\\n\")\n",
    "\n",
    "\n",
    "run_cli_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a4dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, DEAHYUN! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 27, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CL6H2fCjYw3QNP470Yr3yJM7vptVh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--79f706d8-0c96-4921-ba0b-acfb16e881c1-0' usage_metadata={'input_tokens': 27, 'output_tokens': 17, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# chain.invoke({\n",
    "#     \"chat_history\":memory.load_memory_variables({})[\"chat_history\"],\n",
    "#     \"question\":\"What is my name\"\n",
    "#     })\n",
    "\n",
    "invoke_chain(\"My name is DEAHYUN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64848b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Your name is DEAHYUN. If you have any other questions or topics you'd like to talk about, just let me know!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 117, 'total_tokens': 144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CL6HUvaqksVqvpBW5WUbmcKe76DbP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0e203725-ce62-48d6-80d1-feeea77c9115-0' usage_metadata={'input_tokens': 117, 'output_tokens': 27, 'total_tokens': 144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The human introduces themselves as DEAHYUN, and the AI responds by expressing pleasure in meeting them and asking how it can assist today.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is DEAHYUN. How can I help you today?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Your name is DEAHYUN. If there's anything else you'd like to discuss or ask, feel free!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Your name is DEAHYUN. If you have any other questions or topics you'd like to talk about, just let me know!\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"What is my name\")\n",
    "\n",
    "load_memory(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7c049",
   "metadata": {},
   "source": [
    "# ÌòÑÏû¨ÍπåÏßÄ ÏÑ§ÏπòÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4266e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "list =[\"aiohappyeyeballs         2.6.1\",\n",
    "\"aiohttp                  3.12.15\",\n",
    "\"aiosignal                1.4.0\",\n",
    "\"annotated-types          0.7.0\",\n",
    "\"anyio                    4.11.0\",\n",
    "\"anyio                    4.11.0\",\n",
    "\"audioop-lts              0.2.2\",\n",
    "\"certifi                  2025.8.3\",\n",
    "\"charset-normalizer       3.4.3\",\n",
    "\"colorama                 0.4.6\",\n",
    "\"discord.py               2.6.3\",\n",
    "\"distro                   1.9.0\",\n",
    "\"frozenlist               1.7.0\",\n",
    "\"greenlet                 3.2.4\",\n",
    "\"h11                      0.16.0\",\n",
    "\"httpcore                 1.0.9\",\n",
    "\"httpx                    0.28.1\",\n",
    "\"idna                     3.10\",\n",
    "\"jiter                    0.11.0\",\n",
    "\"jsonpatch                1.33\",\n",
    "\"jsonpointer              3.0.0\",\n",
    "\"langchain                0.3.27\",\n",
    "\"langchain-core           0.3.76\",\n",
    "\"langchain-openai         0.3.33\",\n",
    "\"langchain-text-splitters 0.3.11\",\n",
    "\"langsmith                0.4.31\",\n",
    "\"multidict                6.6.4\",\n",
    "\"networkx                 3.5\",\n",
    "\"openai                   1.109.1\",\n",
    "\"orjson                   3.11.3\",\n",
    "\"packaging                25.0\",\n",
    "\"pip                      25.2\",\n",
    "\"propcache                0.3.2\",\n",
    "\"pydantic                 2.11.9\",\n",
    "\"pydantic_core            2.33.2\",\n",
    "\"python-dotenv            1.1.1\",\n",
    "\"pytz                     2025.2\",\n",
    "\"PyYAML                   6.0.3\",\n",
    "\"regex                    2025.9.18\",\n",
    "\"requests                 2.32.5\",\n",
    "\"requests-toolbelt        1.0.0\",\n",
    "\"sniffio                  1.3.1\",\n",
    "\"SQLAlchemy               2.0.43\",\n",
    "\"tenacity                 9.1.2\",\n",
    "\"tiktoken                 0.11.0\",\n",
    "\"tqdm                     4.67.1\",\n",
    "\"typing_extensions        4.15.0\",\n",
    "\"typing-inspection        0.4.1\",\n",
    "\"urllib3                  2.5.0\",\n",
    "\"yarl                     1.20.1\",\n",
    "\"zstandard                0.25.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedb256",
   "metadata": {},
   "source": [
    "# Í≥ºÏ†ú : 30Ïùº ÍπåÏßÄ CLIÏóêÏÑú ÏûëÎèôÌïòÎäî Ï±óÎ¥á ÎßåÎì§Í∏∞\n",
    "# ÌûåÌä∏ : ÏïàÏóê ÏÉùÍ∏¥Í≤å ÎÑàÎ¨¥ jsonÏ≤òÎüº ÏÉùÍπÄ 30Ïùº ÍπåÏßÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-7day-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
